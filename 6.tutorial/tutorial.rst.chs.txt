指导
================

本教程将介绍一些Airflow的基本概念、对象以及它们在编写第一个pipline时的用法。

定义Pipeline的例子
---------------------------

如下就是利用基本方法定义一个pipeline的例子。
不要担心它会很复杂，我们接下来会逐行进行解释。

.. code:: python

    """
    Code that goes along with the Airflow tutorial located at:
    https://github.com/airbnb/airflow/blob/master/airflow/example_dags/tutorial.py
    """
    from airflow import DAG
    from airflow.operators.bash_operator import BashOperator
    from datetime import datetime, timedelta


    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2015, 6, 1),
        'email': ['airflow@airflow.com'],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
    }

    dag = DAG('tutorial', default_args=default_args)

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
        dag=dag)

    t2 = BashOperator(
        task_id='sleep',
        bash_command='sleep 5',
        retries=3,
        dag=dag)

    templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7)}}"
            echo "{{ params.my_param }}"
        {% endfor %}
    """

    t3 = BashOperator(
        task_id='templated',
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
        dag=dag)

    t2.set_upstream(t1)
    t3.set_upstream(t1)


这就是一个DAG的定义文件
--------------------------

有一件事需要考虑(一开始可能不是很直观)，
这个Airflow的Python脚本实际上只是一个指定DAG结构作为代码的配置文件。
实际上这个定义的任务会运行在另一个上下文引用之中。
不同的任务在不同的时间点运行在不同的worker上面，
这意味着脚本在任务之间无法做通信。
注意到这个以后，我们为了达到目的而引入了另一个更高级的特定叫做 ``XCom``。

人们有时会把DAG定义文件看作一个可以进行实际数据处理的地方——但实际上根本不是这样!
这些脚本的目的是为了定义DAG对象。

它需要快速地进行响应和评估任务(执行的频度不是分钟级别的，而是秒级的)，
因为调度程序将周期性地执行它以反映任何的更改。


引入模组
-----------------

一个Airflow的pipeline就是一个Python脚本，这个脚本的作用是为了定义Airflow的DAG对象。
让我们开始引入定义所需要的库。

.. code:: python

    # DAG对象; 我们需要它去实例化一个DAG
    from airflow import DAG

    # Operators; 我们需要利用这个对象去执行流程!
    from airflow.operators.bash_operator import BashOperator

默认参数
-----------------
我们将创建一个DAG和一些任务，我们可以选择将一组参数显式传递给每个任务的构造函数(这可能会变得有些冗余)，
或者(最好地)我们可以定义一个默认参数的字典，我们可以在创建任务时使用它。

.. code:: python

    from datetime import datetime, timedelta

    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2015, 6, 1),
        'email': ['airflow@airflow.com'],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
    }

如果想知道有关BaseOperator's等多的关于变量和具体的执行,
可以去查看 :py:class:``airflow.models.BaseOperator`` 的文档.

另外，请注意，您可以很容易地定义不同的参数集，这些参数将提供不同的作用。
其中一个例子就是您可以在生产和开发环境之间进行不同的配置。


实例化一个DAG
-----------------

我们需要一个DAG对象把我们的任务放进去。
这里我们传递了一个定义为``dag_id``的字符串，它将作为您的DAG的唯一标识符。
我们还传递了我们刚刚定义的默认参数字典，同时也为DAG定义了一个按照一天调度一次的``schedule_interval``。

.. code:: python

    dag = DAG(
        'tutorial', default_args=default_args, schedule_interval=timedelta(1))

任务
-----
当实例化operator对象的时候，会生成任务。
从一个operator实例化出来的对象的过程，被称为一个构造方法。
第一个参数``task_id``是唯一识别一个任务的方式。

.. code:: python

    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
        dag=dag)

    t2 = BashOperator(
        task_id='sleep',
        bash_command='sleep 5',
        retries=3,
        dag=dag)

注意到我们传递了一个BaseOperator特有的参数(``bash_command``)
和所有的operator构造函数中都会有的一个参数(``retries``)。
这比为每个构造函数传递所有的参数要简单很多多。
同时，也注意到在第二个任务里面我们用 ``3`` 覆盖了 ``retries`` 这个参数。

一个任务的优先级规则如下:

1.  有明确传递参数的
2.  在 ``default_args`` 字典存在的并被赋值了的
3.  operator的默认值

一个任务必须包含并且集成参数 ``task_id`` 和 ``owner``，否则Airflow会报错。

利用Jinja作为模板
---------------------
Airflow leverages the power of
`Jinja Templating <http://jinja.pocoo.org/docs/dev/>`_  and provides
the pipeline author
with a set of built-in parameters and macros. Airflow also provides
hooks for the pipeline author to define their own parameters, macros and
templates.

This tutorial barely scratches the surface of what you can do with
templating in Airflow, but the goal of this section is to let you know
this feature exists, get you familiar with double curly brackets, and
point to the most common template variable: ``{{ ds }}``.

.. code:: python

    templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7) }}"
            echo "{{ params.my_param }}"
        {% endfor %}
    """

    t3 = BashOperator(
        task_id='templated',
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
        dag=dag)

注意到 ``templated_command`` 在 ``{% %}`` 块内包含了一段代码逻辑，
引用了变量例如 ``{{ ds }}`` ，在 ``{{ macros.ds_add(ds, 7)}}`` 中调用了一个函数，
在 ``{{ params.my_param }}`` 中引用了一个用户定义的变了。

在 ``BaseOperator`` 中使用的 ``params`` hook允许您将参数和/或对象的字典传递给您的模板。
请花一些时间去了解 ``my_param`` 这个参数是如何在模板中被使用的。

文件也可以当做 ``bash_command`` 的参数进行传递，
例如``bash_command='templated_command.sh'``，
不过这个文件的位置要在pipeline文件的目录内（在这种情况下是``tutorial.py``这个文件）。
这可能是出于多种原因，比如将脚本的逻辑和pipeline代码分隔开，
允许在不同语言的文件中使用适当的代码突出显示，以及灵活地构建pipeline。
还可以定义您的 ``template_searchpath`` ，以指向DAG构造函数调用中的任何文件夹位置。

在模板中可能引用的变量和宏的更多信息，请确保阅读 :ref:`macros` 宏部分。

设置依赖关系
-----------------------
我们有两个不互相依赖的简单的任务。
这里有几种方法可以定义它们之间的依赖关系：

.. code:: python

    t2.set_upstream(t1)

    # This means that t2 will depend on t1
    # running successfully to run
    # It is equivalent to
    # t1.set_downstream(t2)

    t3.set_upstream(t1)

    # all of this is equivalent to
    # dag.set_dependency('print_date', 'sleep')
    # dag.set_dependency('print_date', 'templated')

请注意当执行您的脚本的时候，如果您的脚本里面构成了一个循环或者当一个依赖被引用了一次以上，Airflow会报错。

回顾
-----
好了，现在我们有了一个很好的基础DAG。
这时候您的代码应该看起来是如下的样子：

.. code:: python

    """
    Code that goes along with the Airflow located at:
    http://airflow.readthedocs.org/en/latest/tutorial.html
    """
    from airflow import DAG
    from airflow.operators.bash_operator import BashOperator
    from datetime import datetime, timedelta


    default_args = {
        'owner': 'airflow',
        'depends_on_past': False,
        'start_date': datetime(2015, 6, 1),
        'email': ['airflow@airflow.com'],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
    }

    dag = DAG(
        'tutorial', default_args=default_args, schedule_interval=timedelta(1))

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
        dag=dag)

    t2 = BashOperator(
        task_id='sleep',
        bash_command='sleep 5',
        retries=3,
        dag=dag)

    templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7)}}"
            echo "{{ params.my_param }}"
        {% endfor %}
    """

    t3 = BashOperator(
        task_id='templated',
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
        dag=dag)

    t2.set_upstream(t1)
    t3.set_upstream(t1)

测试
--------

执行脚本
''''''''''''''''''

该是运行一些测试用例的时候了。
首先让我们确保pipeline能够被解析。
让我们保证我们已经将前面的几个步骤的代码保存在 ``airflow.cfg`` 设置的DAGs文件夹中的 ``tutorial.py`` 内。
您默认的DAGs的存储位置在 ``~/airflow/dags`` 。

.. code-block:: bash

    python ~/airflow/dags/tutorial.py

如果这个脚本没有报错，那就证明您的代码和您的Airflow环境没有特别大的问题。

命令行元数据验证
'''''''''''''''''''''''''''''''''
让我们运行一些命令来进一步验证这个脚本。

.. code-block:: bash

    # 打印出目前活跃的DAG列表
    airflow list_dags

    # 打印出使用 "tutorial" 作为dag_id的任务列表
    airflow list_tasks tutorial

    # 打印出在tutorial的DAG中任务的层次结构
    airflow list_tasks tutorial --tree


测试
'''''''
让我们通过指定一个日期并运行现有的任务实例来进行测试。
指定的日期利用 ``execution_date`` 来进行定义，它会模拟在特定的日期和时间运行您的任务或者dag：

.. code-block:: bash

    # command layout: command subcommand dag_id task_id date

    # 测试 print_date
    airflow test tutorial print_date 2015-06-01

    # 测试 sleep
    airflow test tutorial sleep 2015-06-01

现在还记得我们早些时候利用模板都做了什么？
让我们看看通过执行这个命令以后，模板都呈献了些什么：

.. code-block:: bash

    # 测试 templated
    airflow test tutorial templated 2015-06-01

这会在一个详细的事件日志上显示结果，并将运行bash命令的结果并打印出来。

注意到 ``airflow test`` 的命令在本地运行了任务实例，并在标准输出（屏幕）上显示这些日志结果，
不去在意依赖关系，并且不将任何状态信息写入数据库中（运行，成功，失败等）。
它只允许测试单个任务实例。

回填
''''''''
从各种情况上来看它运行的不错，
以，让我们来运行一个回填（原文叫backfill，我也不知道如何翻译，所以暂时叫做回填了）。
``回填`` 将考虑到您的依赖项，将日志发送到文件中，并与数据库进行通信从而记录期间的状态。
如果您启动了一个web服务，您可以跟踪它的进度。
如果您有兴趣在你的回填过程中，在显示屏上跟踪这些进度，
那么您就可以利用 ``airflow webserver`` 启动一个web服务。

请注意如果您使用 ``depends_on_past=True``，单独的任务实例依赖于前一个任务实例的成功，
除非它自己定义了start_date，这个依赖项才会被忽略。

在本文中的日期范围是 ``start_date`` 和一个可能的 ``end_date`` 来定义的，
这两个值在dag中的任务实例里被用来填充运行调度。

.. code-block:: bash

    # 当然我们也可以选择使用一个debug模式在后台启动一个web服务
    # airflow webserver --debug &

    # 在一个时间段内启动一个backfill
    airflow backfill tutorial -s 2015-06-01 -e 2015-06-07

接下来做什么?
-------------
就是这样，您已经编写、测试并且回填了您的第一个初始的Airflow的pipeline。
将您的代码合并到一个有调度管理器的代码库中，这样可以启动任务并在每天执行它。

如下是一些您在接下来的部分要做的一些事情：

* 对用户界面进行深入的访问——点击所有的东西！
* 保持阅读文档! 特别是如下的部分:

    * 命令行接口
    * 任务执行体
    * 宏命令

* 写下你第一个pipeline吧！
